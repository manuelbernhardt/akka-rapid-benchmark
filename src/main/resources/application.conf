akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    provider = cluster
  }

  remote {

    watch-failure-detector {
      heartbeat-interval = 5 s
      threshold = 12.0
      acceptable-heartbeat-pause = 15 s
      expected-response-after = 6 s
    }

    artery {
      transport = tcp

      canonical.hostname = "127.0.0.1"
      canonical.hostname = ${?HOSTNAME}
      canonical.port     = 2552

      tcp {
        connection-timeout = 5 seconds
      }

      advanced {
        # so this one is _really_ important for a large cluster to form
        # rapid will assign new observers / subjects after each consensus round
        # to fit in the expander graph
        # this will:
        # - affect most nodes a little bit (2 more associations created)
        # - affect a subset of the nodes a lot (20 new associations after a round)
        # as it turns out, a t3.small instance cannot handle more than 70 associations before becoming unstable
        stop-idle-outbound-after = 20 s

        system-message-resend-interval = 1 s
        handshake-retry-interval = 1 second
        inject-handshake-interval = 1 second

        inbound-max-restarts = 30
        outbound-max-restarts = 50
        outbound-restart-backoff = 1 second
        outbound-restart-timeout = 5 seconds

        # should be at most 1 MB
        maximum-frame-size = 2M
      }

    }
  }

  cluster {

    failure-detector {
      # 1 second by default, but we want to run 10000 nodes with t3.small instances and they only have 2 vCPUs and really strange networking
      # heartbeats are exchanged between K nodes (10 by default), from observer to subject
      # so their reach is only local, between nodes
      heartbeat-interval = 5 s
      threshold = 12.0
      acceptable-heartbeat-pause = 15 s
      expected-response-after = 5 s
    }

    rapid {

      use-consistent-hash-broadcasting = true

      # broadcasters override this via the JVM parameters
      act-as-consistent-hash-broadcaster = false

      # For regular nodes and broadcasting nodes, there's no need to create large batches as there shouldn't be that many
      # edge DOWN alerts
      batching-window = 200 millis

      # fall back to paxos consensus
      # make this too low and a gazillion nodes will attempt full paxos
      consensus-fallback-timeout-base-delay = 20 seconds

      messaging {
        # This is way beyond what it should be (it should be 1 second)
        # But we're running this at scale on small machines and at 10000 nodes the largest message reach 1MB
        # Additionally, the broadcasting scheme in use will involve at 3 hops for most messages, adding to the latency
        # Therefore we are being patient
        default-timeout = 10 seconds
        default-retries = 5
      }
    }
  }

  http {
    server {
      enable-http2 = false
      max-connections = 100000
      idle-timeout = 15 s
    }
    client {
      idle-timeout = 15 s
    }
  }

  coordinated-shutdown.exit-jvm = on

}

